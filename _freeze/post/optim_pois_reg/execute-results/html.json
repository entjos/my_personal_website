{
  "hash": "961af1d6467b91ae3bbf9b109b182772",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimisation of a Poisson survival model using Optimx in R\"\ndescription: \"In this blog post, we will fit a Poisson regression model by maximising its likelihood function using `optimx()` in `R`. As an example we will use the lung cancer data set included in the `{survival}` package.\"\nauthor: \"Joshua Philipp Entrop\"\ndate: '2020-07-22'\ncategories: [Optimisation, R]\ntags: [R, survival analysis, manual optimisation]\ndraft: FALSE\n---\n\n\n[R Code](https://www.joshua-entrop.com/rcode/optim_pois_reg.txt){.btn .btn-outline-primary .btn role=\"button\"}\n\nIn this blog post, we will fit a Poisson regression model by maximising its likelihood function using `optimx()` in `R`. As an example we will use the lung cancer data set included in the `{survival}` package. The data set includes information on 228 lung cancer patients from the North Central Cancer Treatment Group (NCCTG). Specifically, we will estimate the survival of lung cancer patients by sex and age using a simple Poisson regression model. You can download the code that I will use throughout post [here](https://www.joshua-entrop.com/rcode/optim_pois_reg.txt). The general survival function $S(t)$ for our model can be specified as\n\n$$ S(t) = \\exp(-\\lambda t) $$\n\nwhere the hazard function $h(t)$ is equal to\n\n$$ h(t) = \\lambda = \\exp(\\alpha + \\beta_1  x_{female} + \\beta_2  x_{age}). $$ To get started we first need to load all the packages that we will need for our estimations and set up the data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Prefix -------------------------------------------------------------------\n\n# Remove all files from ls\nrm(list = ls())\n\n# Loading packages\nrequire(survival)\nrequire(flexsurv)\nrequire(optimx)\nrequire(numDeriv)\nrequire(dplyr)\nrequire(tibble)\nrequire(car)\n\n# 2. Loading dataset ----------------------------------------------------------\n\n#Reading the example data set lung from the survival package\nlung <- as.data.frame(survival::lung)\n\n#Recode dichotomous vairables\nlung$female <- ifelse(lung$sex == 2, 1, 0)\nlung$status_n <- ifelse(lung$status == 2, 1, 0)\n```\n:::\n\n\nAt this point we would usually call `survreg()` or `flexsurvreg()` to fit our Possion model. However, in this post we will use the likelihood function of our Possion regression model together with `optimx()` from the `{optimx}` package instead. For this we first need to find the likelihood function for our model and then use `optimx()` to find the values for our parameters, that maximise our likelihood function.\n\nThe log likelihood ($\\ln L_i$) for a survival model can be specified as\n\n$$ \\ln L_i = d_i \\ln h(t_i) + \\ln S(t_i). $$ Where $d_i$ indicates whether the $i^{th}$ subject experienced an event (1) or did not experience an event (0) during follow up and $t_i$ is the time from the beginning of follow up until censoring.\n\nTo obtain the log likelihood function for our Possion model we can substitute $h(t)$ and $S(t)$ with our previously defined hazard and survival function respectively. Thus, we get the following equation for our log likelihood\n\n$$\\ln L_i = d_i \\ln \\lambda - \\lambda t_i $$\n\nwhere $\\lambda$ is defined as mentioned above.\n\nThe next step is now to write our likelihood function as a function in R, which can be maximised by `optimx()`. Please keep in mind, that `optimx()` by default minimises the function we pass to it. However, in our case we need to find the maximum of our likelihood function. To yield the estimates, that maximise our function we can just ask `optimx()` to minimise the negative of our likelihood. For more information on setting up the likelihood function for `optimx()` or `optim()` please take a look at [this](https://www.joshua-entrop.com/post/optim_linear_reg.html) earlier blog post.\n\nLets set up our likelihood function in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3. Define log-likelihood function for Poisson regression model --------------\nnegll <- function(par){\n  \n  #Extract guesses for alpha, beta1 and beta2\n  alpha <- par[1]\n  beta1 <- par[2]\n  beta2 <- par[3]\n  \n  #Define dependent and independent variables\n  t  <- lung$time\n  d  <- lung$status_n\n  x1 <- lung$female\n  x2 <- lung$age\n  \n  #Calculate lambda\n  lambda <- exp(alpha + beta1 * x1 + beta2 * x2)\n  \n  #Estimate negetive log likelihood value\n  val <- -sum(d * log(lambda) - lambda * t)\n  \n  return(val)\n}\n```\n:::\n\n\nTo improve the optimisation we can further pass the gradient function of our likelihood function to our `optimx()` call. After partially deriving $L_i$ for $\\alpha$ and $\\beta_i$ we yield the two following equations for the gradient of $L_i$.\n\n$$ \\sum d_i - \\lambda_i t_i  = 0$$\n\n$$ \\sum d_i x_{ij} - \\lambda_i x_{ij} t = 0$$\n\nGiven these gradient equations we can now define our gradient function in R. For this we need to create a function, that returns the gradient for each of our unknown parameters. Since we have three unknown parameters our gradient function will return a vector `gg` with three values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4. Define gradient function for Poisson regression model -------------------\nnegll.grad <- function(par){\n  \n  #Extract guesses for alpha and beta1\n  alpha <- par[1]\n  beta1 <- par[2]\n  beta2 <- par[3]\n  \n  #Define dependent and independent variables\n  t  <- lung$time\n  d  <- lung$status_n\n  x1 <- lung$female\n  x2 <- lung$age\n  \n  #Create output vector\n  n <- length(par[1])\n  gg <- as.vector(rep(0, n))\n  \n  #Calculate pi and xb\n  lambda <- exp(alpha + beta1 * x1 + beta2 * x2)\n  \n  #Calculate gradients for alpha and beta1\n  gg[1] <- -sum(d - lambda * t)\n  gg[2] <- -sum(d * x1 - lambda * x1 * t)\n  gg[3] <- -sum(d * x2 - lambda * x2 * t)\n  \n  return(gg)\n}\n```\n:::\n\n\nWe can compare the results of our gradient function with the results from the `grad()` function included in the `{numDeriv}` package, before we begin with the optimisation of our functions. This is just a check to be sure our gradient function works properly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4.1 Compare gradient function with numeric approximation of gradient ========\n# compare gradient at 0, 0, 0\nmygrad <- negll.grad(c(0, 0, 0))\nnumgrad <- grad(x = c(0, 0, 0), func = negll)\n\nall.equal(mygrad, numgrad)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nLooks like our gradient functions does a good job. Now that we have all the functions and information we need for our optimisation, we can call `optimx()` and pass our functions to it.\n\nThe output of `optimx()` provides us with estimates for our coefficients and information regarding whether the optimisation algorithm converged (`convcode == 0`) besides the maximum value of the negative log likelihood obtained by the different algorithms. Hence, it is useful to sort the results by `convcode` and `value`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 5. Find maximum of log-likelihood function ----------------------------------\n\n# Passing names to the values in the par vector improves readability of results\nopt <- optimx(par = c(alpha = 0, beta_female = 0, beta_age = 0), \n              fn = negll,\n              gr = negll.grad,\n              hessian = TRUE,\n              control = list(trace = 0, all.methods = TRUE))\n\n# Show results for optimisation alogrithms, that convergered (convcode == 0)\nsummary(opt, order = \"value\") %>%\n  rownames_to_column(\"algorithm\") %>% \n  filter(convcode == 0) %>% \n  select(algorithm, alpha, beta_female, beta_age, value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    algorithm     alpha beta_female   beta_age    value\n1      nlminb -6.840606  -0.4809343 0.01561870 1156.099\n2        BFGS -6.840627  -0.4809436 0.01561907 1156.099\n3    L-BFGS-B -6.840636  -0.4809316 0.01561902 1156.099\n4 Nelder-Mead -6.832428  -0.4814582 0.01547911 1156.099\n```\n\n\n:::\n:::\n\n\nThe summary of our `optimx()` call shows, that the `nlminb` algorithm yielded the best result. Lets see if this result is equal to the results we will get, if we use `flexsurvreg` from the `{flexsurv}` package to fit our desired model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 6. Estimate regression coeficents using flexsurvreg -------------------------\npois_model <- flexsurvreg(Surv(time, status_n == 1) ~ female + age, \n                          data = lung,\n                          dist = \"exp\")\n\n# 7. Comparing results from optimx and flexsurvreg ----------------------------\npois_results <- unname(coef(pois_model))\ncoef_opt <- coef(opt)\n\nlapply(1:nrow(coef_opt), function(i){\n  \n  opt_name <- attributes(coef_opt)$dimnames[[1]][i]\n  \n  mle_pois1 <- (coef_opt[i, 1] - pois_results[1])\n  mle_pois2 <- (coef_opt[i, 2] - pois_results[2])\n  mle_pois3 <- (coef_opt[i, 3] - pois_results[3])\n  \n  mean_dif <- mean(mle_pois1, mle_pois2, mle_pois3, na.rm = TRUE)\n  \n  data.frame(opt_name, mean_dif)\n  \n}) %>% \n  bind_rows() %>% \n  filter(!is.na(mean_dif)) %>% \n  mutate(mean_dif = abs(mean_dif)) %>% \n  arrange(mean_dif)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     opt_name     mean_dif\n1      nlminb 2.678650e-07\n2        BFGS 2.091779e-05\n3    L-BFGS-B 2.911668e-05\n4 Nelder-Mead 8.178948e-03\n5          CG 6.816256e+00\n6      Rvmmin 6.840606e+00\n```\n\n\n:::\n:::\n\n\nThe mean difference between our estimates and the estimates obtained by using `flexsurvreg()` are close to zero. Seems like our optimisation using the log likelihood did a good job.\n\nHowever, the result obtained with `flexsurvreg()` provided us with estimates for the standard errors (SEs) of our hazard estimates, too. Since the measurement of uncertainty is at the heart of statistics, I think it is worthwhile to obtain the SEs for our estimates with the information provided by our `optimx()` call. For a more detailed discussion on how this is done please take a look at one of my previous blog posts [here](https://www.joshua-entrop.com/post/optim_logit_reg_se.html).\n\nLet's obtain the SEs for our model by using the results from our `optimx()` call and compare them with the SEs obtained by `flexsurvreg()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 8. Estimate the standard error ----------------------------------------------\n\n#Extract hessian matrix for nlminb optimisation\nhessian_m <- attributes(opt)$details[\"nlminb\", \"nhatend\"][[1]]\n\n# Estimate SE based on hession matrix\nfisher_info <- solve(hessian_m)\nprop_se  <- sqrt(diag(fisher_info))\n\n# Compare the estimated SE from our model with the one from the flexsurv model\n# Note use res.t to get the estimates on the reale scale without transformaiton\nses <- data.frame(se_nlminb = prop_se, \n                  se_felxsurvreg = pois_model$res.t[, \"se\"]) %>%\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         se_nlminb se_felxsurvreg\nrate   0.587477415     0.58747740\nfemale 0.167094278     0.16709429\nage    0.009105681     0.00910568\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(ses[, \"se_nlminb\"], ses[, \"se_felxsurvreg\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Mean relative difference: 3.697145e-08\"\n```\n\n\n:::\n:::\n\n\nLooks like we got nearly equal results. Let us use these information and estimate the 95% confidence intervals (CIs) for our estimates now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 9. Estimate 95%CIs using estimation of SE -----------------------------------\n\n# Extracting estimates from nlminb optimisaiton\ncoef_test <- coef(opt)[\"nlminb\",]\n\n# Compute 95%CIs\nupper <- coef_test + 1.96 * prop_se\nlower <- coef_test - 1.96 * prop_se\n\n# Print 95%CIs\ndata.frame(Estimate = coef_test, \n           CI_lower = lower, \n           CI_upper = upper, \n           se       = prop_se)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate     CI_lower    CI_upper          se\nalpha       -6.8406062 -7.992061931 -5.68915046 0.587477415\nbeta_female -0.4809343 -0.808439062 -0.15342949 0.167094278\nbeta_age     0.0156187 -0.002228433  0.03346584 0.009105681\n```\n\n\n:::\n:::\n\n\nOne usual way to plot the results of our estimation is plotting the survival function $S(t)$. Since, uncertainty is important I also want to plot the CI for our survival function. To obtain estimates for the SE of the survival function $S(t)$ is a little bit more complicated. However, the amazing `deltaMethod()` function included in the `{car}` package makes it fairly easy to obtain estimates for the SEs. We just need to provide `deltaMethod()` with a vector of our coefficients, our covariance matrix and the computation for which we would like to obtain the SEs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 10. Plot survival curve with 95%-CI -----------------------------------------\n\n# 10.1 Use Delta Method to compute CIs across time of follow-up ===============\n\n# Get coefficents for nlminb optimisation\nnlminb_coef <- coef(opt)[\"nlminb\", ]\n\n# Compute CIs for a 60 year old female across follow-up time\nsurv_optim_female <- lapply(as.list(seq(0.01, 1000.01, 10)), function(t){\n  \n  g <- paste(\"exp(-exp(alpha + beta_female + 60 * beta_age) *\", t, \")\")\n  \n  fit <- deltaMethod(nlminb_coef, g, solve(hessian_m))\n  \n  data.frame(time     = t,\n             estimate = fit[, \"Estimate\"],\n             ci_low   = fit[, \"2.5 %\"],\n             ci_up    = fit[, \"97.5 %\"])\n  \n}) %>% \n  bind_rows()\n```\n:::\n\n\nWe can now use these information to plot our survival curve $S(t)$ together with a grey shaded area that indicates the CIs for our survival function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 10.2 Plot survival curve with CIs ===========================================\nplot(surv_optim_female$time,\n     surv_optim_female$estimate,\n     ylim = c(0, 1),\n     type = \"n\",\n     xlab = \"Time in Days\",\n     ylab = \"S(t)\",\n     main = \"Survival after lung cancer \\n for 60 year old females\")\npolygon(c(surv_optim_female$time, rev(surv_optim_female$time)),\n        c(surv_optim_female$ci_low, rev(surv_optim_female$ci_up)),\n        border = NA,\n        col = \"grey\")\nlines(surv_optim_female$time,\n     surv_optim_female$estimate)\nlegend(0, 0.15,\n       fill = \"grey\",\n       \"95% CI\")\n```\n\n::: {.cell-output-display}\n![](optim_pois_reg_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nTo sum it up, in this blog post we learned how to fit a Possion regression model using the log likelihood function in R instead of going the usual way of calling `survreg()` or `flexsurvreg()`. I think doing this is a good way of gaining a deeper understanding of how estimates for regression models are obtained. In my next post I will take this a step further and show how we can fit a Weibull regression model in R using the log likelihood function in combination with `optimx()`.\n",
    "supporting": [
      "optim_pois_reg_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}