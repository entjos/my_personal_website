{
  "hash": "9e062d36388286a5a8942efeb1d719e2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Estimating Standard Errors for a Logistic Regression Model optimised with Optimx in R\"\ndescription: \"In my previous I estimated the point estimates for a logistic regression model using `optimx()` from the `optimx` package in `R`. In this post I would like to contine with this model an try to find the standard error (SE) for the derived estimates.\"\nauthor: \"Joshua Philipp Entrop\"\ndate: '2020-06-25'\noutput: html_document\ncategories: [Optimisation, R]\ntags: [R, logistic regression, manual optimisation]\n---\n\n\n[R Code](https://www.joshua-entrop.com/rcode/optim_logit_reg_se.txt){.btn .btn-outline-primary .btn role=\"button\"}\n\n\n\n\n\nIn my <a href=\"/optim_logit_reg.html\">last post</a> I estimated the point estimates for a logistic regression model using `optimx()` from the `optimx` package in `R`. In this post I would like to contine with this model an try to find the standard error (SE) for the derived estimates. Uncertainty is probably the most important quantity in statistics and therefore I think it is worthwhile to look a lite bit more into this. However, before, we can start with the estimation of the SEs, I would ask you to run the code for deriving the point estimates for the logistic regression using `optimx()`, which you can find <a href=\"https://www.joshua-entrop.com/rcode/optim_logit_reg.txt\">here</a>. This will be the starting point for our further calculations.\n\nWhen I searched for an answer to solve the problem of estimating the SE using the output of `optimx()` in R, I came across this quite old <a href=\"https://stat.ethz.ch/pipermail/r-help/2004-February/046272.html\">email</a> from 2004 on the R-help email list and a <a href=\"https://stats.stackexchange.com/questions/27033/in-r-given-an-output-from-optim-with-a-hessian-matrix-how-to-calculate-paramet\">discussion</a> on stackexchange. Basically it says that we can compute the covariance matrix as the inverse of the negative of the Hessian matrix. Given our estimated covariance matrix, we can then estimate the SE as the square root of the diagonal elements of our covariance matrix.\n\nSo, lets try to implement this in `R`. First we need to extract the Hessian matrix from our `optimx()` result object. Note, that you need to set the option `hessian = TRUE` in your `optimx()` call. This asks `optimx()` to estimate the Hessian matrix for the different optimization algorithms and allows us to obtain this information after the optimization is finished. In the example below, I only obtain the Hessian matrix for the optimization algorithm `Rcgmin`, since it showed the best fit compared to the results from the `glm()` model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 7. Estimate the standard error ----------------------------------------------\n\n#Extract hessian matrix for Rcgmin optimisation\nhessian_m <- attributes(opt)$details[\"Rcgmin\", \"nhatend\"][[1]]\n```\n:::\n\n\nAfter we extracted the Hessian matrix, we can follow the procedure described above. Also note, that I used the Hessian matrix, instead of the negative Hessian matrix in my example. When I used the negative Hessian matrix, I got negative values for the diagonal values of the inverse. Hence, I was not able to obtain the squared root of these values. Also, I obtained the correct SEs using this approach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate se based on hession matrix\nfisher_info <- solve(hessian_m)\nprop_se  <- sqrt(diag(fisher_info))\n```\n:::\n\n\nNow were we obtained our estimates for the SEs, it would be interesting to compare them with the results of a `glm()` call, that tries to fit the same model as we do.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare the estimated se from our model with the one from the glm\nses <- data.frame(se_Rcgmin = prop_se, \n                  se_glm = tidy(glm_model)$std.error) %>%\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   se_Rcgmin     se_glm\n1 0.69888433 0.69884208\n2 0.01065624 0.01065569\n3 0.37177192 0.37176526\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(ses[,\"se_Rcgmin\"], ses[, \"se_glm\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Mean relative difference: 4.57513e-05\"\n```\n\n\n:::\n:::\n\n\nThe differences between the estimates of the SEs using the Hessian matrix and the `glm()` model are very small. It seems like our approach did a fairly good job. Hence, we can now use our SE estimates to compute the 95%CIs of our point estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 8. Estimate 95%CIs using estimation of SE -----------------------------------\n\n# Extracting estimates from Rcgmin optimisaiton\ncoef_test <- coef(opt)[\"Rcgmin\",]\n\n# Compute 95%CIs\nupper <- coef_test + 1.96 * prop_se\nlower <- coef_test - 1.96 * prop_se\n\n# Print 95%CIs\ndata.frame(coef_test, lower=lower, upper=upper, se = prop_se)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     coef_test        lower       upper         se\np1 -3.05669070 -4.426503993 -1.68687741 0.69888433\np2  0.02758409  0.006697859  0.04847032 0.01065624\np3 -0.01131098 -0.739983952  0.71736199 0.37177192\n```\n\n\n:::\n:::\n\n\nCombining this and my previous post on <a href=\"https://www.joshua-entrop.com/post/optim_logit_reg.html\">optimizing a logistic regression using optimx()</a>, we were able to more or less manually obtain the results of a logistic regression model, that we would commonly obtain using the `glm()` function.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}