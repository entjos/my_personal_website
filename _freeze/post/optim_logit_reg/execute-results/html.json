{
  "hash": "035007572b35b0ea31782feea60e212c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimisation of a Logistic Regression Model using Optimx in R\"\ndescription: \"This blog post explains how one can manually optimise a logistic regression model using a maximum likelihood estimation (MLE) method in R without the use pre-defined functions.\"\nauthor: \"Joshua Philipp Entrop\"\ndate: '2020-05-27'\ncategories: [Optimisation, R]\ntags: [R, logistic regression, manual optimisation]\noutput: html_document\n---\n\n\n[R Code](https://www.joshua-entrop.com/rcode/optim_logit_reg.txt){.btn .btn-outline-primary .btn role=\"button\"}\n\nIn my <a href=\"https://www.joshua-entrop.com/post/optim_linear_reg.html\">last post</a> I used the `optim()` command to optimise a linear regression model. In this post, I am going to take that approach a little further and optimise a logistic regression model in the same manner. Thanks to John C. Nash, I got a first glimpse into the world of optimisation functions in `R`. His book showed me how important it is to compare the results of different optimisation algorithms. Where I used the `base` optimisation function `optim()` in my last post, I will use `optimx()` from the `optimx` package in this post. The `optimx` package and function were developed by Nash and colleagues as a wrapper of the `stats::optim()` function. There are numerous advantages in using `optimx()` instead of `optim()`. In my opinion, among the most important is an easier comparison between different optimisation methods. In case someone is more interested in the variety of optimisation functions and problems that come with them, I can warmly recommend John C. Nash's book <a href=\"https://www.wiley.com/en-us/Nonlinear+Parameter+Optimization+Using+R+Tools-p-9781118569283\"><i>Nonlinear Parameter Optimisation Using R Tools</i></a>.\n\nHowever, coming back to my main focus: the optimisation of a logistic regression model using the `optimx()` function in R. For this, I would like to use the `icu` data set from the package `aplore3`. The data set contains data from 200 patients in an intensive care unit (ICU) and provides information whether the patient survived their stay or died. The particular question I would like to take a look at is whether the probability of dying during the ICU stay $P(Y = 1)$ is related to age $(x_1)$ and sex $(x_2)$. In order to do so, I firstly would like to load the data set and set up our variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Prefix -------------------------------------------------------------------\n\n# Remove all files from ls\nrm(list = ls())\n\n# Loading packages\nrequire(aplore3)\nrequire(optimx)\nrequire(numDeriv)\nrequire(dplyr)\n\n# 2. Loading dataset ----------------------------------------------------------\n\n#Reading the example data set icu from the package aplore3\nicu <- as.data.frame(icu)\nicu$sta_n <- ifelse(icu$sta == \"Died\", 1, 0)\nicu$female <- ifelse(icu$gender == \"Female\", 1, 0)\n```\n:::\n\n\nThe specific model that I would like to use is:\n\n$$ P(Y|x_1, x_2) \\sim \\alpha + \\beta_1  x_1 + \\beta_2  x_2$$ Using the logistic link-function we can find a linear function for the right side of the equation.\n\n$$ \\ln \\Bigg[ \\frac{P(Y|x_1, x_2)}{1 - P(Y|x_1, x_2)} \\Bigg] = \\alpha + \\beta_1  x_1 + \\beta_2  x_2$$\n\nFor this model, I would like to find the values $\\alpha$, $\\beta_1$ and $\\beta_2$ that maximize the log-likelihood function and hence, provides the best fit to our empirical data provided in the `icu` data set. Therefore, we also need to define the log-likelihood function for our logistic regression model. According to <a href=\"https://www.wiley.com/en-us/Applied+Logistic+Regression%2C+3rd+Edition-p-9780470582473\">Hosmer and Lemeshow</a> the log-likelihood function for a logistic regression model can be defined as\n\n$$ \\sum_{i = 1}^{n}(y_i - \\ln(\\pi_i)) + (1 - y_i) * \\ln(1 - \\pi_i)). $$ Where $\\pi$ is defined using the sigmoid function as\n\n$$ P(Y|x_1, x_2) = \\pi = \\frac{\\exp(\\alpha + \\beta_1  x_1 + \\beta_2  x_2)}{1 + \\exp(\\alpha + \\beta_1  x_1 + \\beta_2  x_2)}. $$\n\nFor the optimisation in R we need to define the log-likelihood function as a function in `R`. Additionally, we need to add the constrain $0 < \\pi < 1$ to our like-likelihood function, since we are interested in a probability $\\pi$ which needs to be in the range between $0$ and $1$. We can use an `if` statement in `R` to include our constrain to our R function. For all parameter values that return a value of $\\pi$ that is out of the bounds, we set the value to a very high number, for instance $10^{200}$. Using these high numbers for values outside the bounds, the optimisation algorithm will dismiss these parameter values from our solutions. Note that we calculate `-sum()`, since we want to find the maximum of the log-likelihood function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3. Define log-likelihood function for logistic regression model -------------\n# (see applied logistic regression)\nnegll <- function(par){\n  \n  #Extract guesses for alpha and beta1\n  alpha <- par[1]\n  beta1 <- par[2]\n  beta2 <- par[3]\n  \n  #Define dependent and independent variables\n  y  <- icu$sta_n\n  x1 <- icu$age\n  x2 <- icu$female\n  \n  #Calculate pi and xb\n  xb <- alpha + beta1 * x1 + beta2 * x2\n  pi <- exp(xb) / (1 + exp(xb))\n  \n  #Set high values for 0 < pi < 1\n  if(any(pi > 1) | any(pi < 0)) {\n    val <- 1e+200\n  } else {\n    val <- -sum(y * log(pi) + (1 - y) * log(1 - pi))\n  }\n  val\n}\n```\n:::\n\n\nAdditionally to our log-likelihood function, it is also useful to specify the gradient function for our log-likelihood. This is not necessary for all optimisation algorithms, however, it improves the testing for convergence. Hence, we can obtain better estimates by also supplying the gradient function. According to Hosmer and Lemeshow, the gradient function of the log-likelihood function is defined as\n\n$$\ng_\\alpha(\\pi) = \\sum(y_i - \\pi_i)\n$$ $$\ng_{x_j}(\\pi) = \\sum(x_{ji} * (y_i - \\pi_i)).\n$$\n\nIn our case we yield 3 gradient functions\n\n$$\ng_\\alpha(\\pi) = \\sum(y_i - \\pi_i)\n$$ $$\ng_{x_1}(\\pi) = \\sum(x_{1_i} * (y_i - \\pi_i))\n$$\n\n$$\ng_{x_2}(\\pi) = \\sum(x_{2_i} * (y_i - \\pi_i)).\n$$\n\nWe can then use these 3 functions to calculate the gradients in R. Also here we need to use `-sum()` for the gradient functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4. Define fradient function for logistic regression model -------------------\n# (see applied logistic regression)\nnegll.grad <- function(par){\n  \n  #Extract guesses for alpha and beta1\n  alpha <- par[1]\n  beta1 <- par[2]\n  beta2 <- par[3]\n  \n  #Define dependent and independent variables\n  y  <- icu$sta_n\n  x1 <- icu$age\n  x2 <- icu$female\n  \n  #Create output vector\n  n <- length(par[1])\n  gg <- as.vector(rep(0, n))\n  \n  #Calculate pi and xb\n  xb <- alpha + beta1 * x1 + beta2 * x2\n  pi <- exp(xb) / (1 + exp(xb))\n  \n  #Calculate gradients for alpha and beta1\n  gg[1] <- -sum(y - pi)\n  gg[2] <- -sum(x1 * (y - pi))\n  gg[3] <- -sum(x2 * (y - pi))\n  \n  return(gg)\n}\n```\n:::\n\n\n`R` also provides functions to estimate a numerical approximation of the gradient function. One of these function is `grad()` from the `numDeriv` package. It is useful to double check your analytic gradient function using one of these numerical approximations. Since, `optimx()` uses the `grad()` function for doing this, we are also going to use this function\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4.1 Compare gradient function with numeric approximation of gradient ========\n# compare gradient at 0, 0, 0\nmygrad <- negll.grad(c(0, 0, 0))\nnumgrad <- grad(x = c(0, 0, 0), func = negll)\n\nall.equal(mygrad, numgrad)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nWe see, that the results from our analytic gradient function are identical to the results using the `grad()` function. So we can proceed and use the `optimx()` function to find the maximum of our log-likelihood function. As a first guess we use $0$ as initial value for all unknown parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4. Find maximum of log-likelihood function ----------------------------------\nopt <- optimx(par = c(alpha  = 0,\n                      beta_1 = 0, \n                      beta_2 = 0), \n              fn = negll, \n              gr = negll.grad, \n              control = list(trace = 0, \n                             all.methods = TRUE))\n\n# print reulsts of optimisation\n# remove not needed information for purpose of presentation \nsummary(opt, order = \"convcode\") %>% \n  select(-value, -niter, -gevals, -fevals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                alpha     beta_1      beta_2 convcode  kkt1 kkt2 xtime\nNelder-Mead -3.055741 0.02756878 -0.01126589        0 FALSE TRUE  0.00\nL-BFGS-B    -3.056762 0.02758493 -0.01130317        0  TRUE TRUE  0.00\nnlm         -3.056691 0.02758409 -0.01131100        0  TRUE TRUE  0.01\nnlminb      -3.056690 0.02758409 -0.01131159        0  TRUE TRUE  0.00\nRcgmin      -3.056691 0.02758409 -0.01131098        1  TRUE TRUE  0.03\nRvmmin       0.000000 0.00000000  0.00000000       21 FALSE TRUE  0.00\nBFGS               NA         NA          NA     9999    NA   NA  0.00\nCG                 NA         NA          NA     9999    NA   NA  0.00\nspg                NA         NA          NA     9999    NA   NA  0.00\nucminf             NA         NA          NA     9999    NA   NA  0.00\nnewuoa             NA         NA          NA     9999    NA   NA  0.05\nbobyqa             NA         NA          NA     9999    NA   NA  0.00\nnmkb               NA         NA          NA     9999    NA   NA  0.00\nhjkb               NA         NA          NA     9999    NA   NA  0.00\n```\n\n\n:::\n:::\n\n\nA value of $0$ in the `convcode` column of the output indicates, that the algorithm converged. Even though multiple algorithms converged and gave us a value for our three unknown parameters, they all provide slightly different estimates. Therefore, I think it would be interesting to compare our estimates with the estimates from the commonly used `glm()` function. Below I wrote a small function that estimates the mean differences in the estimates from the different optimisation methods and the `glm` model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 5. Estimate regression coeficents using glm ---------------------------------\nglm_model <- glm(sta_n ~ age + female, \n                 data = icu,\n                 family = binomial(link = \"logit\"))\n\n# Print coefficents\ncoef(glm_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)         age      female \n-3.05669068  0.02758409 -0.01131098 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 6. Comparing results from optimx and glm ------------------------------------\nglm_results <- unname(coef(glm_model))\ncoef_opt <- coef(opt)\n\nlapply(1:nrow(coef_opt), function(i){\n    \n    optimisation_algorithm <- attributes(coef_opt)$dimnames[[1]][i]\n\n    mle_glm1 <- (coef_opt[i, \"alpha\" ] - glm_results[1])\n    mle_glm2 <- (coef_opt[i, \"beta_1\"] - glm_results[2])\n    mle_glm3 <- (coef_opt[i, \"beta_2\"] - glm_results[3])\n    \n    mean_difference <- mean(mle_glm1, mle_glm2, mle_glm3, na.rm = TRUE)\n    \n    data.frame(optimisation_algorithm, mean_difference)\n    \n  }) %>% \n    bind_rows() %>% \n  filter(!is.na(mean_difference)) %>% \n  mutate(mean_difference = abs(mean_difference)) %>% \n  arrange(mean_difference)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  optimisation_algorithm mean_difference\n1                 Rcgmin    1.887799e-08\n2                    nlm    5.856574e-08\n3                 nlminb    2.071622e-07\n4               L-BFGS-B    7.134885e-05\n5            Nelder-Mead    9.493966e-04\n6                 Rvmmin    3.056691e+00\n```\n\n\n:::\n:::\n\n\nThis shows that the `Rcgmin` algorithm yield the most similar results to the estimates from the `glm` model. However, most of the algorithms in the table provide estimates similar to the estimates from the `glm` model, which indicates that our optimisation of the logistic regression model using the log-likelihood function and the gradient function worked out well.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}